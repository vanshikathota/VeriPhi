{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fNZZilUV_L4r",
    "outputId": "438d9ef6-3ce6-4055-fd38-8b572dbd41e6"
   },
   "outputs": [],
   "source": [
    "+!pip install -U bitsandbytes\n",
    "!pip install -U transformers\n",
    "!pip install -U peft\n",
    "!pip install -U accelerate\n",
    "!pip install -U datasets\n",
    "!pip install -U trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLzgZ14X_rMs"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdy76XXtrKoc"
   },
   "outputs": [],
   "source": [
    "secret_hf = \"YOUR_HUGGINGFACE_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9oI6bPHzpDW",
    "outputId": "43e6ecd7-c4e0-47f6-f130-5f6b284688b5"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token $secret_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zCg612_rKoc"
   },
   "outputs": [],
   "source": [
    "base_model = \"base_model_name\"  # e.g., \"meta-llama/Llama-2-7b-hf\"\n",
    "new_model = \"new_model_name\"  # e.g., \"your_username/your_model_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "c8b2987701e444939bf7447f6465d59c",
      "6fddb3e9c00e44778f957f7286e7b7a3",
      "56b9eddf9f0c403fb0780bacdf9110ce",
      "ae929511ce924e5088a68dc3275b6bb0",
      "32461010b4ec45db9b4488ed404e8eff",
      "d98fee1459b14de0aa7e4b0a37f13f37",
      "846e6a099f294d5eb8a6974bfe0e3353",
      "d806fe68c43546b2a83e05db32a91f6f",
      "0d38bcead51a4371bc54ab10e271916c",
      "a1fac905332e44249da7f20e6bea7a45",
      "d44a0dc77ecd4706bde5776f13bac2f3",
      "7d301da56190455bbc2f300cd0a23bf3",
      "971ccbbac2e941a792cec045d1a60477",
      "94c7392dd3e1461883c12f2ebaa25775",
      "e6ae2320e4e64056ba1bf3fbdc31cba7",
      "bfe5632d1a3f4b0f8419512a4705c362",
      "d75b0d9d534c4107976d612c35ded79e",
      "f3c9afccf9bd4640ae6d851adcabb40a",
      "ce25d3f39d9249f6a9fbc29943b7d7a7",
      "4792b5f875e44d519668b6d26bcc8d8b",
      "6ca1558e553645d5b260d1cf2e44955f",
      "e6387916e0a04ceb833ff241ccc2989d",
      "3363401b4b62457eab12c5057e5aac76",
      "b9e8a5aa7e134a6683a03efa8afe3131",
      "4d718d2eb13c44a68956832c30cc7949",
      "1fe133da1d1f495e84b70fb5e421a7e9",
      "5219edfd91f2471c98dff6dc8140c968",
      "bdae3ef35a7a412e8a0a3413ed749c51",
      "31dded845d8b4bf0a3858207e028e83b",
      "fdc34efd09b343b1ab92f2602d1bbc4c",
      "c32e769e3c774d988e36c6ad09204902",
      "44a8de7e39914c36b6448d31b3561272",
      "1ad1bad9dbfe4734bc7b973a23d5a94d"
     ]
    },
    "id": "XzF2UjPvTBag",
    "outputId": "6654e35b-0f7d-4ec1-ed58-9c731f2e5a52"
   },
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"shailja/Verilog_GitHub\", split=\"train[:1000]\")  # Adjust the split as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752,
     "referenced_widgets": [
      "34dccc9096b441c18704581d358039e4",
      "1a16b536862e4dc0a36e0d2f3479a3b5",
      "5325267b0cbb4afcb2d01a583d35f125",
      "0efb254ce037400186cd6ba765eed3cb",
      "e3aea4f1a75c4ca58adfd28ae2d75b30",
      "5b9c46c3d54b46e7a9af90a1277e22e3",
      "834ead82009d47fcb5c3c81508944616",
      "c9dde6234aca484db4c94115036c0bb6",
      "e9411e32aade4161a60e235c7605f600",
      "931e771d32c749e9a03e7a94999828f1",
      "9d2ae880376d4b26a8523d7d19c25b1f"
     ]
    },
    "id": "StJKGiDDHzdk",
    "outputId": "34d2ff91-111c-403b-ac57-b9de4e2b0b8a"
   },
   "outputs": [],
   "source": [
    "# Load base model\n",
    "from transformers import BitsAndBytesConfig ,AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4_ExVZnrKoe"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, PeftModel, get_peft_model\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense',\n",
    "        'fc1',\n",
    "        'fc2',\n",
    "    ]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peOnLAAhS0y1"
   },
   "outputs": [],
   "source": [
    "#Hyperparamter\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./batch-48-47001-48000\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSRTLtP8rKof"
   },
   "outputs": [],
   "source": [
    "# Setting sft parameters\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length= 2048,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "-zQUBhOJrKof",
    "outputId": "b458db44-6483-499c-87f8-781c4ce81608"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKgZBEGVS5a2"
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ukYCqu9kOrz2",
    "outputId": "94d0b090-4103-4f15-c7c1-8a7a700ea1b4"
   },
   "outputs": [],
   "source": [
    "output_dir = \"D:/fine-tuned-model\"  # Directory to save your model\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "trainer.tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461,
     "referenced_widgets": [
      "9dcfdf59dc604723af4d70817f086e54",
      "07d3c9a1bcb74ba2894f721a0830c2bf",
      "f9e766eb078b42c686d7c70f8b54e57f",
      "4c0b0faf2d57425ea1dfef39ac9af2b6",
      "12f2514c0f1e4c25924b1e4b9e8c79b7",
      "06dae1dfeab9480f86950aeb2e518b36",
      "af7b1395638d41e3bc7e68dad74a162d",
      "b8a274f2e75044d3a4d7881f2de8b868",
      "58e1d3fd048d4fe19a6d109867d164d8",
      "8d24332188bb4109a999b805ce20aa45",
      "6a19ba4940214e59a05eddab076e77ca",
      "c84478f05b3c416fa448a733d4e734f4",
      "d363ec59e7954260877c062cfd94f94e",
      "ff7d7ddbe14749ef90b6ddbfd23728f7",
      "dee4e5ec0cc24292805df146e0a3d2b8",
      "6d907f86eee147de8446a75c692b4420",
      "0476424e9b874953a9f531a7ee337be7",
      "49839ee0ab27484f8a0c865e25e285a3",
      "4fe1167450f442ffbc4f4f42afca03c7",
      "66009bd9256143c2b13ba932084334c4",
      "db786a375e5e4155bf84e710cd1ee1d6",
      "c4f548484e91497ca7ac7df289fbf22d",
      "7b88044ffeb64b1084fdde393a936314",
      "b848a166ca364622967ca46225fa76fa",
      "1cbe8d07e77a4cdd89054f23d616ac42",
      "f54be4fea5844a418e336bbf3dc0df8c",
      "5883d286eeeb48bea37e9108fd1f88b0",
      "f6ba08176a6b4fb781ae088bb422bf84",
      "805d7e0a6a504158a025ae379f154810",
      "ad365d881ea945d4adda34872846be9f",
      "07bff53026f5419caed957458bf64742",
      "0ccb7dd6dfe54f3f9aa6083fd4e93133",
      "c9c5134ddd1342faa7719deca6fbfc2c",
      "e7b9d7511bec4bc39c5bdb1bb8435c1d",
      "ff30efa29e7f448e8edfa0f5f377f736",
      "2cf229c071294830afbcd370ce195207",
      "c09e591ca94a4312b67f6f1fd0f11ceb",
      "99ce1df8455f4469aa7ca26f3e705b3c",
      "814a9bda4876422c9391fcd0ed3103d7",
      "6d16ce7cfadc4e1b85608cff03a52eeb",
      "5050161addb04f04b7eee552f0766ece",
      "df0992f64e3d4468974cb27bb9807ce0",
      "270bfab941b94dc69d39d374dc512748",
      "f7184ad6d8f24de7a95d730e68877026",
      "c3473faf616b4967b927666f415c59f7",
      "3bb22a8ee8214eb6a7ec8eaf9274d261",
      "27ba1b22184f468b8a39c4a7eb72f61e",
      "54703e149b814f0fa14914ef7ec4b5c7",
      "b1c1853249f04f738bb8ea120ed30482",
      "8a363ab5af514644813832bab83e2e5a"
     ]
    },
    "id": "Srncv7xwOUkx",
    "outputId": "464c015a-a7e6-45cd-8124-174036097c9d"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Authenticate with Hugging Face (if not already done)\n",
    "notebook_login()\n",
    "\n",
    "# Save the model locally first\n",
    "trainer.model.save_pretrained(new_model)\n",
    "\n",
    "# Push the model to the Hub, providing your Hugging Face token if necessary\n",
    "trainer.push_to_hub(new_model, token='YOUR_HUGGINGFACE_TOKEN') # Replace 'YOUR_HUGGINGFACE_TOKEN' with your actual token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7vHP41ITQPb"
   },
   "outputs": [],
   "source": [
    "prompt = \"<|system|><|user|><|assistant|>\"\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
    "result = pipe(prompt)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "id": "Bi9cPUayQmub",
    "outputId": "c27550f0-bc5a-4d08-ee27-e70056a81cff"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "model_path = 'meghnareddy90/batch-7-6001-7000'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,trust_remote_code=True)\n",
    "\n",
    "# Prepare your prompt\n",
    "prompt = \"module my_module(input clk, input reset, output reg [7:0] out);\\n\"\n",
    "\n",
    "# Tokenize and generate response\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output_sequences = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    max_length=150,\n",
    "    temperature=0.7,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "\n",
    "# Decode and print the generated code\n",
    "generated_code = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "print(generated_code)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6212.204373,
   "end_time": "2023-12-07T12:11:36.804994",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-07T10:28:04.600621",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
